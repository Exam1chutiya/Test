<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Deep Learning Exam – Answers</title>
<style>
body { font-family: Arial, sans-serif; line-height: 1.6; margin: 30px; }
h1, h2, h3 { color: #2c3e50; }
code { background: #f4f4f4; padding: 2px 4px; }
.section { margin-bottom: 40px; }
</style>
</head>
<body>

<h1>Deep Learning – Answer Sheet</h1>

<div class="section">
<h2>SECTION A (Memory Based Questions)</h2>
<ol>
<li>
<strong>What is the Vanishing Gradient Problem?</strong><br>
In deep neural networks, during backpropagation the gradients can become extremely small as they are propagated backward through many layers. When this happens, the earlier layers learn very slowly or stop learning completely. This issue is called the vanishing gradient problem.
</li>

<li>
<strong>Name any two commonly used activation functions in deep learning.</strong><br>
Examples:
<ul>
<li>ReLU (Rectified Linear Unit)</li>
<li>Sigmoid</li>
<li>Tanh</li>
<li>Leaky ReLU</li>
</ul>
</li>

<li>
<strong>What is the purpose of the Adam optimizer?</strong><br>
Adam (Adaptive Moment Estimation) is an optimization algorithm that adapts the learning rate for each parameter using first-order (mean) and second-order (variance) moments of the gradients. It speeds up training and improves convergence stability.
</li>

<li>
<strong>What is the difference between Convolution and Pooling in CNNs?</strong><br>
Convolution extracts features by applying learnable filters that compute weighted sums over local regions.<br>
Pooling reduces spatial dimensions (downsampling) by summarizing local regions (e.g., using max or average), which reduces parameters and overfitting.
</li>

<li>
<strong>What are Autoencoders?</strong><br>
Autoencoders are neural networks trained to reconstruct their input. They consist of an encoder that compresses data into a latent space and a decoder that reconstructs it. They are used for dimensionality reduction, denoising, anomaly detection, etc.
</li>
</ol>
</div>

<div class="section">
<h2>SECTION B (Concept Based Questions)</h2>

<h3>1. Explain the Gradient Descent algorithm.</h3>
<p>
Gradient Descent is an optimization algorithm used to minimize a loss function. It updates model parameters in the direction opposite to the gradient of the loss:
</p>
<p><code>θ := θ − η · ∇L(θ)</code></p>
<p>
where θ are parameters, η is the learning rate, and ∇L is the gradient of loss.
</p>

<h3>2. Max Pooling and Average Pooling</h3>
<p>Given matrix:</p>
<pre>
X = [
 [4 1 3 2],
 [2 8 1 0],
 [5 2 7 3],
 [1 4 6 2]
]
</pre>

<p>Top-left 2×2 window:</p>
<pre>
W = [
 [4 1],
 [2 8]
]
</pre>

<p><strong>(a) Max-pooled value:</strong></p>
<p>
max(4, 1, 2, 8) = <strong>8</strong>
</p>

<p><strong>(b) Average-pooled value:</strong></p>
<p>
Average = (4 + 1 + 2 + 8) / 4 = 15 / 4 = <strong>3.75</strong>
</p>

<h3>3. What is a GAN? Explain how Generator and Discriminator work.</h3>
<p>
A Generative Adversarial Network (GAN) consists of two networks:
</p>
<ul>
<li><strong>Generator</strong> – creates fake data from random noise</li>
<li><strong>Discriminator</strong> – tries to distinguish real data from fake data</li>
</ul>
<p>
They are trained together in a minimax game until the generator produces realistic data.
</p>

<h3>4. Explain the working of an LSTM network.</h3>
<p>
LSTM (Long Short-Term Memory) networks are RNNs designed to capture long-term dependencies. They use gates:
</p>
<ul>
<li>Forget gate – decides what to discard</li>
<li>Input gate – decides what new info to store</li>
<li>Output gate – controls hidden state output</li>
</ul>
<p>
This prevents vanishing gradients and allows long-range memory.
</p>

<h3>5. What is Batch Normalization? Explain its advantages.</h3>
<p>
Batch Normalization normalizes inputs of each layer to have zero mean and unit variance within a mini-batch. Advantages include:
</p>
<ul>
<li>Faster training</li>
<li>Stabilized learning</li>
<li>Reduced internal covariate shift</li>
<li>Acts as a regularizer</li>
</ul>
</div>

<div class="section">
<h2>SECTION C (Analytical Based Questions)</h2>
<p>
For RNN forward pass and Backpropagation Through Time (BPTT), the process is:
</p>
<ol>
<li>Compute hidden state at each time step using previous state.</li>
<li>Compute outputs at each time step.</li>
<li>Compute total loss across the sequence.</li>
<li>Backpropagate gradients through time to update weights.</li>
</ol>
<p>
Gradients are computed with the chain rule using recurrent dependencies.
</p>
</div>

<div class="section">
<h2>SECTION D (Case Study / Application Based Question)</h2>

<h3>2-2-1 Neural Network Numerical Problem</h3>

<p><strong>Given:</strong></p>
<ul>
<li>Inputs: x₁ = 1, x₂ = 0</li>
<li>Weights:
<ul>
<li>w₁₁ = 0.1, w₁₂ = 0.2</li>
<li>w₂₁ = 0.3, w₂₂ = 0.4</li>
</ul></li>
<li>Hidden → Output weights:
<ul>
<li>v₁ = 0.5, v₂ = 0.6</li>
</ul></li>
<li>Biases:
<ul>
<li>b<sub>h1</sub> = 0.1, b<sub>h2</sub> = 0.1</li>
<li>b<sub>o</sub> = 0.1</li>
</ul></li>
<li>Activation: sigmoid σ(z)</li>
<li>Target y = 1</li>
<li>Learning rate η = 0.1</li>
</ul>

<h3>Forward Pass</h3>

<p><strong>Hidden Layer:</strong></p>
<p>
net₁ = 0.1·1 + 0.3·0 + 0.1 = 0.2 → h₁ = σ(0.2) ≈ 0.5498<br>
net₂ = 0.2·1 + 0.4·0 + 0.1 = 0.3 → h₂ = σ(0.3) ≈ 0.5744
</p>

<p><strong>Output Layer:</strong></p>
<p>
net<sub>o</sub> = 0.5·0.5498 + 0.6·0.5744 + 0.1 ≈ 0.8196<br>
ŷ = σ(0.8196) ≈ <strong>0.694</strong>
</p>

<p><strong>Loss:</strong></p>
<p>
L = ½ (ŷ − y)² ≈ 0.0468
</p>

<h3>Backpropagation</h3>

<p><strong>Output delta:</strong></p>
<p>
δ<sub>o</sub> = (ŷ − y)·ŷ·(1−ŷ) ≈ −0.065
</p>

<p><strong>Gradients:</strong></p>
<ul>
<li>dL/dv₁ = δ<sub>o</sub>·h₁ ≈ −0.0357</li>
<li>dL/dv₂ = δ<sub>o</sub>·h₂ ≈ −0.0373</li>
<li>dL/db₀ = δ<sub>o</sub></li>
</ul>

<p><strong>Hidden deltas:</strong></p>
<p>
δ<sub>h1</sub> ≈ −0.0080<br>
δ<sub>h2</sub> ≈ −0.0095
</p>

<p><strong>Input-layer gradients:</strong></p>
<ul>
<li>dL/dw₁₁ ≈ −0.0080</li>
<li>dL/dw₁₂ ≈ −0.0095</li>
<li>dL/dw₂₁ = 0</li>
<li>dL/dw₂₂ = 0</li>
</ul>

<h3>Weight Updates (η = 0.1)</h3>

<ul>
<li>v₁ = 0.5 + 0.0036 ≈ <strong>0.5036</strong></li>
<li>v₂ = 0.6 + 0.0037 ≈ <strong>0.6037</strong></li>
<li>b₀ = 0.1 + 0.0065 ≈ <strong>0.1065</strong></li>
<li>w₁₁ = 0.1 + 0.0008 ≈ <strong>0.1008</strong></li>
<li>w₁₂ = 0.2 + 0.0010 ≈ <strong>0.2010</strong></li>
<li>w₂₁ = 0.3</li>
<li>w₂₂ = 0.4</li>
<li>b<sub>h1</sub> ≈ <strong>0.1008</strong></li>
<li>b<sub>h2</sub> ≈ <strong>0.1010</strong></li>
</ul>

<p>
These are the updated parameters after one training iteration.
</p>

</div>

</body>
</html>

