<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
   

    @media (max-width: 600px) {
      nav ul {
        flex-direction: column;
      }

      .content div {
        width: 90%;
      }
    }
  </style>
</head>
<body>
  <header>
    <h1>
        Q D1-  <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KITNA LIKHEGI RAND.</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2 {
            color: #333;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        table, th, td {
            border: 1px solid #ddd;
        }
        th, td {
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
        }
    </style>
</head>
<body>
  Q13: Formulate a payoff matrix and derive equilibrium using the best-response method<br>
Payoff Matrix:<br>
Manager/Employee Extra Effort (E) Minimal Effort (M)<br>
High-Responsibility Task (H) (6, 5) (1, 2)<br>
Low-Responsibility Task (L) (4, 3) (2, 1)<br>
Best-Response Method:<br>
Step 1: Identify the employee's best response for each manager's decision.<br>
If the manager assigns H:<br>
Payoffs for employee: Extra Effort (5) > Minimal Effort (2). Best response: E.<br>
If the manager assigns L:<br>
Payoffs for employee: Extra Effort (3) > Minimal Effort (1). Best response: E.<br>
Step 2: Identify the manager's best response for each employee's decision.<br>
If the employee chooses E:<br>
Payoffs for manager: H (6) > L (4). Best response: H.<br>
If the employee chooses M:<br>
Payoffs for manager: H (1) < L (2). Best response: L.<br>
Step 3: Find the Nash Equilibrium.<br>
The pair of strategies where both players are best responding to each other is (H, E) with payoffs (6, 5).<br>
Q14: Design an extensive form game and derive equilibrium using backward induction<br>
Extensive Form Game (Tree Representation):<br>
Stage 1 (Manager's Decision):<br>
Manager chooses between H (High-Responsibility) or L (Low-Responsibility).<br>
Stage 2 (Employee's Decision):<br>
After observing the task, the employee chooses between E (Extra Effort) or M (Minimal Effort).<br>
Payoffs at Each Leaf Node:<br>
H → E: (6, 5)<br>
H → M: (1, 2)<br>
L → E: (4, 3)<br>
L → M: (2, 1)<br>
Backward Induction:<br>
Employee's Decision:<br>
For H: E (5) > M (2) → Best response: E.<br>
For L: E (3) > M (1) → Best response: E.<br>
Manager's Decision:<br>
Knowing the employee will choose E, the manager compares payoffs:<br>
H → E: 6 vs. L → E: 4. Best response: H.<br>
Nash Equilibrium (Subgame Perfect Equilibrium):<br>
The equilibrium strategy is (H, E) with payoffs (6, 5).<br>
<h2> Q14 (Payoff Matrix & Equilibrium Strategy using Best-Response Method)</h2>
    
    <h3>Step 1: Constructing the Payoff Matrix (Normal Form)</h3>
    <p>The given game is sequential, but we can convert it into <strong>normal form (payoff matrix)</strong> by listing each player's strategies.</p>

    <h3>Players and Their Strategies:</h3>
    <p><strong>Player 1 (P1) has two choices initially:</strong><br>
    - <strong>A</strong> (ends game, payoff <strong>(4,3)</strong>)<br>
    - <strong>B</strong> (continues game, Player 2 moves)</p>

    <p><strong>Player 2 (P2) has two choices if P1 plays B:</strong><br>
    - <strong>X</strong> (ends game, payoff <strong>(2,5)</strong>)<br>
    - <strong>Y</strong> (continues game, Player 1 moves again)</p>

    <p><strong>If Player 2 chooses Y, Player 1 has two more choices:</strong><br>
    - <strong>C</strong> (payoff <strong>(3,2)</strong>)<br>
    - <strong>D</strong> (payoff <strong>(1,4)</strong>)</p>

    <h3>Payoff Matrix:</h3>
    <table border="1">
        <tr>
            <th>Player 1 Strategy</th>
            <th>Player 2 chooses X</th>
            <th>Player 2 chooses Y → P1 chooses C</th>
            <th>Player 2 chooses Y → P1 chooses D</th>
        </tr>
        <tr>
            <td><strong>A</strong></td>
            <td>(4,3)</td>
            <td>(4,3)</td>
            <td>(4,3)</td>
        </tr>
        <tr>
            <td><strong>B → X</strong></td>
            <td>(2,5)</td>
            <td>(2,5)</td>
            <td>(2,5)</td>
        </tr>
        <tr>
            <td><strong>B → Y → C</strong></td>
            <td>(3,2)</td>
            <td>(3,2)</td>
            <td>-</td>
        </tr>
        <tr>
            <td><strong>B → Y → D</strong></td>
            <td>(1,4)</td>
            <td>-</td>
            <td>(1,4)</td>
        </tr>
    </table>

    <h3>Step 2: Finding the Best-Response & Equilibrium Strategy</h3>
    <p>- Player 1 prefers A (4,3) over B → X (2,5), but if Player 2 chooses Y, then Player 1 prefers C (3,2) over D (1,4).<br>
    - Player 2 prefers X (5 for them) if Player 1 plays B but would prefer Y if they believe Player 1 will choose C (2 for them vs. 4 in case of D).</p>

    <h3>Equilibrium Strategy:</h3>
    <p>- Player 1 chooses A (4,3), as it guarantees a better outcome than the uncertain continuation.<br>
    - If forced into B, Player 1 should choose C (3,2) over D (1,4).<br>
    - Player 2 should choose X if Player 1 plays B.</p>

    <p><strong>Thus, the equilibrium strategy is (A, -) with payoff (4,3).</strong></p>

    <hr>

    <h2>Q13 (Extensive Form Game & Subgame Perfect Nash Equilibrium using Backward Induction)</h2>

    <h3>Step 1: Drawing the Extensive Form Game (Game Tree)</h3>
    <p>- Player 1 starts and chooses between A and B.<br>
    - If A, game ends → Payoff (4,3).<br>
    - If B, Player 2 chooses X or Y.<br>
    - If X, game ends → Payoff (2,5).<br>
    - If Y, Player 1 chooses C or D.<br>
    - If C, payoff (3,2).<br>
    - If D, payoff (1,4).</p>

    <h3>Step 2: Solving via Backward Induction</h3>
    <p>1. At Player 1’s final decision point (after Y):<br>
    - C (3,2) > D (1,4) → So Player 1 will choose C.<br>
    2. At Player 2’s decision point (after B):<br>
    - Compare X (2,5) vs. Y (3,2).<br>
    - Player 2 prefers X (5 vs. 2).<br>
    3. At Player 1’s first decision point:<br>
    - A (4,3) vs. B → X (2,5).<br>
    - Player 1 prefers A (4,3).</p>

    <p><strong>Thus, the subgame perfect Nash equilibrium (SPNE) is (A, -) with payoff (4,3).</strong></p>

    <hr>

    <h2> Q12 (Observability in Reinforcement Learning & Partial Observability Impact)</h2>

    <h3>1. Role of Observability in RL:</h3>
    <p>- Full observability: The agent receives complete information about the environment state, enabling <strong>accurate learning</strong>.<br>
    - Partial observability: The agent gets <strong>incomplete information</strong>, requiring probabilistic models to estimate missing data.</p>

    <h3>2. Impact of Partial Observability:</h3>
    <p>- Slower learning: The agent must infer hidden state information.<br>
    - Suboptimal decisions: Incorrect state estimations lead to <strong>suboptimal strategies</strong>.<br>
    - Increased exploration: Agents must explore more to learn hidden aspects of the environment.</p>

    <h3>3. Methods to Handle Hidden States:</h3>
    <p>- <strong>POMDPs</strong> (Partially Observable Markov Decision Processes): Use probability distributions over hidden states.<br>
    - <strong>Recurrent Neural Networks (RNNs)</strong>: Store historical information to infer missing states.<br>
    - <strong>Belief State Estimation</strong>: Maintain a probability distribution over possible states.</p>

    <hr>

    <h2>Q11 (Perfect Bayesian Equilibrium for Signaling Game)</h2>

    <h3>Step 1: Understanding the Game</h3>
    <p>- Nature assigns Player 1’s type (T1 or T2) with probabilities (T1 = 0.7, T2 = 0.3).<br>
    - Player 1 observes their type and sends a message (E or F).<br>
    - Player 2 sees the message and chooses A or B.</p>

    <h3>Step 2: Find Beliefs & Expected Payoff</h3>
    <p>- If Player 1 is type T1 → Choosing E gives R = 3, Choosing F gives R = 6 → Player 1 (T1) prefers F.<br>
    - If Player 1 is type T2 → Choosing E gives R = 1, Choosing F gives R = 6 → Player 1 (T2) also prefers F.</p>

    <p><strong>Equilibrium Strategy: (F, E) with expected payoff (6, 3).</strong></p>
  <h1>Game Theory Exam Solutions</h1>

    <hr>

    <h2>Section B: Concept-Based Questions</h2>

    <h3>Q6: Minimax Regret Approach</h3>
    <p>A tech company is deciding between three Machine Learning models (A, B, and C) based on dataset quality (Small, Medium, Large). The given payoffs are in percentage points.</p>

    <h3>Step 1: Regret Table Calculation</h3>
    <p>Regret is calculated as:  
    <strong>Regret = Max Payoff in Column - Actual Payoff</strong></p>

    <table border="1">
        <tr>
            <th>Model</th>
            <th>Small Dataset</th>
            <th>Medium Dataset</th>
            <th>Large Dataset</th>
        </tr>
        <tr>
            <td>Model A</td>
            <td>85</td>
            <td>75</td>
            <td>90</td>
        </tr>
        <tr>
            <td>Model B</td>
            <td>70</td>
            <td>80</td>
            <td>90</td>
        </tr>
        <tr>
            <td>Model C</td>
            <td>65</td>
            <td>85</td>
            <td>95</td>
        </tr>
    </table>

    <p><strong>Step 2: Compute Maximum Regret for Each Model</strong></p>
    <p>- Model A: Max regret = 10  
    - Model B: Max regret = 15  
    - Model C: Max regret = 5</p>

    <p><strong>Final Decision:</strong> Using Minimax Regret, we choose Model C as it has the least maximum regret (5).</p>

    <hr>

    <h3>Q7: Expected Opportunity Loss (EOL) Approach</h3>
    <p>Given dataset probabilities: Small (0.3), Medium (0.5), Large (0.2).</p>

    <h3>Step 1: Expected Loss Calculation</h3>
    <p>- EOL(Model A) = (0.3 * 10) + (0.5 * 5) + (0.2 * 0) = 4.5  
    - EOL(Model B) = (0.3 * 15) + (0.5 * 10) + (0.2 * 5) = 9.5  
    - EOL(Model C) = (0.3 * 5) + (0.5 * 0) + (0.2 * 0) = 1.5</p>

    <p><strong>Final Decision:</strong> Model C has the lowest EOL (1.5), so the company should choose Model C.</p>

    <hr>

    <h3>Q8: Multi-Agent Reinforcement Learning (MARL) Key Elements</h3>
    <p>- <strong>Agents:</strong> Multiple decision-makers interacting in an environment.<br>
    - <strong>States:</strong> The current conditions in which agents operate.<br>
    - <strong>Actions:</strong> Moves taken by agents in response to states.<br>
    - <strong>Rewards:</strong> Feedback signals for actions.<br>
    - <strong>Policies:</strong> Strategies guiding actions.</p>

    <p><strong>Contribution to Optimal Strategies:</strong> MARL enables learning through interactions, helping agents to converge toward Nash equilibrium in multi-agent settings.</p>

    <hr>

    <h3>Q9: Minimax-Maximin Theorem</h3>
    <p>- Minimax strategy: Minimizing the maximum possible loss.<br>
    - Maximin strategy: Maximizing the minimum guaranteed payoff.<br>
    - For a zero-sum game, Minimax = Maximin, ensuring equilibrium.</p>

    <hr>

    <h2>Section A: Memory-Based Questions</h2>

    <h3>Q1: Backward Induction in Extensive Form Games</h3>
    <p>Backward induction involves solving the game from the last decision nodes backward to the first, ensuring rational choices at each stage.</p>

    <hr>

    <h3>Q2: Difference Between Pure and Mixed Strategies</h3>
    <p>- <strong>Pure Strategy:</strong> A player always chooses the same action.<br>
    - <strong>Mixed Strategy:</strong> A player randomizes over actions.</p>

    <p><strong>Example:</strong> In Rock-Paper-Scissors, choosing Rock always is a pure strategy, while choosing randomly among Rock, Paper, and Scissors is a mixed strategy.</p>

    <hr>

    <h3>Q3: Importance of Nash Equilibrium in Game Theory</h3>
    <p>Nash Equilibrium ensures that no player can benefit by unilaterally deviating from their strategy, making it a stable solution concept.</p>

    <hr>

    <h3>Q4: Components of an Extensive Form Game</h3>
    <p>- <strong>Players:</strong> Decision-makers in the game.<br>
    - <strong>Strategies:</strong> Action plans for players.<br>
    - <strong>Payoffs:</strong> Rewards for different strategy combinations.<br>
    - <strong>Information Sets:</strong> Knowledge about prior moves.<br>
    - <strong>Decision Nodes:</strong> Points where choices are made.</p>

    <hr>

    <h3>Q5: How Reinforcement Learning Integrates with Game Theory</h3>
    <p>- RL helps agents learn optimal strategies over time.<br>
    - In multi-agent systems, RL converges to equilibrium strategies.<br>
    - Examples: Q-learning in strategic decision-making.</p>

    <hr>

    <footer>
        <p>Prepared for Game Theory Exam - AI & Machine Learning</p>
    </footer>
 PASTE HERE 
  </body>
</html>
