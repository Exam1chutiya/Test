<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Healthcare & Machine Learning — Study Guide</title>
  <style>
    body{font-family:Arial,Helvetica,sans-serif;line-height:1.6;margin:20px;color:#111}
    header{border-bottom:4px solid #2f6f8f;padding-bottom:10px;margin-bottom:18px}
    h1{color:#0b4f6c}
    h2{color:#1a6b5a}
    .card{background:#f7fbfc;border:1px solid #e3eef0;padding:14px;margin:12px 0;border-radius:8px}
    .example{font-style:italic;color:#333}
    footer{margin-top:22px;font-size:0.9em;color:#555}
    code{background:#eaeef0;padding:2px 4px;border-radius:4px}
  </style>
</head>
<body>


  
SECTION A — Memory-Based Questions
Q1. Define stop list in keyword extraction.

A stop list is a predefined collection of high-frequency, low-information words (such as “the,” “is,” “and,” “of”) that are filtered out during keyword extraction because they do not contribute semantic value to document representation.

Q2. Outline the use of NMF in email classification.

Non-negative Matrix Factorization (NMF) decomposes the term–document matrix into two non-negative matrices:

W (term–topic matrix)

H (topic–document matrix)

In email classification:

NMF uncovers latent topics representing typical email themes (spam offers, promotions, personal messages, etc.).

Each email is represented as a mixture of discovered topics.

These topic-based features improve classification accuracy for spam detection, clustering, and category tagging.

Q3. Define precision, recall, and F1-score.

Precision = TP / (TP + FP)
Measures how many retrieved items are actually relevant.

Recall = TP / (TP + FN)
Measures how many relevant items were successfully retrieved.

F1-Score = 2 × (Precision × Recall) / (Precision + Recall)
Harmonic mean of precision and recall.

Q4. True or False

(i) TF-IDF identifies important keywords by computing term frequency and inverse document frequency values.
True

(ii) F1-score is the harmonic means of precision and recall.
True

Q5. State two advantages of the Tucker1 method in multilingual clustering.

Tucker1 performs multi-way decomposition, enabling simultaneous modeling of terms, documents, and languages.

It captures cross-lingual semantic relationships, improving clustering quality even with vocabulary mismatches across languages.

SECTION B — Concept-Based Questions
Q6. TF-IDF keyword importance determination. (6 marks)

TF-IDF assigns importance using:

Term Frequency (TF):
Measures how frequently a term appears within a document.
TF(t, d) = count of t in d.

Inverse Document Frequency (IDF):
Measures how unique a term is across the corpus.
IDF(t) = log(N / df(t)).

TF × IDF Score:
High when a term is frequent in the current document but rare across all documents.
This suppresses common terms (stop words) and highlights discriminative terms.

Thus TF-IDF identifies keywords by balancing local frequency and global rarity.

Q7. Compare LSA and LDA. (6 marks)
Aspect	LSA (Latent Semantic Analysis)	LDA (Latent Dirichlet Allocation)
Model Type	Linear algebra (SVD)	Probabilistic generative model
Topic Representation	Deterministic latent dimensions	Distribution over words (topics)
Document Representation	Weighted combination of singular vectors	Distribution over topics
Assumptions	Words co-occur in latent semantic space	Documents are mixtures of latent topics
Output	Dense, continuous embeddings	Sparse, interpretable topic distributions
Strength	Captures global structure efficiently	Produces interpretable topics
Q8. K-Means Clustering Calculation

Documents:
D1(1,0,1), D2(2,1,1), D3(4,3,2), D4(5,4,3), D5(0,1,0)

Initial centroids:
C1 = D1 = (1,0,1)
C2 = D3 = (4,3,2)
C3 = D5 = (0,1,0)

(a) Distances of each document to each centroid

Use Euclidean distance.

I will compute concisely:

Distances to C1 (1,0,1):
D1: 0
D2: √[(2−1)² + (1−0)² + (1−1)²] = √2
D3: √[(4−1)² + (3−0)² + (2−1)²] = √19
D4: √[(5−1)² + (4−0)² + (3−1)²] = √32
D5: √[(0−1)² + (1−0)² + (0−1)²] = √3

Distances to C2 (4,3,2):
D1: √19
D2: √[(2−4)² + (1−3)² + (1−2)²] = √9 = 3
D3: 0
D4: √[(5−4)² + (4−3)² + (3−2)²] = √3
D5: √[(0−4)² + (1−3)² + (0−2)²] = √24

Distances to C3 (0,1,0):
D1: √3
D2: √[(2−0)² + (1−1)² + (1−0)²] = √5
D3: √[(4−0)² + (3−1)² + (2−0)²] = √24
D4: √[(5−0)² + (4−1)² + (3−0)²] = √50
D5: 0

(b) Cluster assignments

Assign to nearest centroid:

D1 → C1

D2 → C1 (√2 < 3 < √5)

D3 → C2

D4 → C2

D5 → C3

Clusters:
C1: {D1, D2}
C2: {D3, D4}
C3: {D5}

(c) Updated centroids after iteration 1

C1 = mean(D1, D2)
= ((1+2)/2, (0+1)/2, (1+1)/2)
= (1.5, 0.5, 1)

C2 = mean(D3, D4)
= ((4+5)/2, (3+4)/2, (2+3)/2)
= (4.5, 3.5, 2.5)

C3 = D5 = (0,1,0)

Q9. Explain content-based spam classification. (6 marks)

Feature Extraction:
Bag-of-Words, TF-IDF, or embeddings identify suspicious tokens (lottery, free, win).

Training:
A supervised ML model (Naive Bayes, SVM, Logistic Regression) is trained on labeled spam/ham data.

Prediction:
New emails are transformed into feature vectors and classified based on learned patterns.

Common algorithms:

Multinomial Naive Bayes

Support Vector Machines (SVM)

Q10. Cosine Similarity Calculation

Matrix (rewritten):
T1: D1=2 D2=1 D3=0 D4=3
T2: 2 0 1 0
T3: 2 1 3 1
T4: 2 1 1 2

So document vectors:

D1 = (2,2,2,2)
D2 = (1,0,1,1)
D3 = (0,1,3,1)
D4 = (3,0,1,2)

Compute norms:
‖D1‖ = √(16) = 4
‖D2‖ = √3
‖D3‖ = √11
‖D4‖ = √14

(i) Cos(D1, D2)

Dot = 2·1 + 2·0 + 2·1 + 2·1 = 6
cos = 6 / (4√3)

(ii) Cos(D1, D3)

Dot = 2·0 + 2·1 + 2·3 + 2·1 = 10
cos = 10 / (4√11)

(iii) Cos(D1, D4)

Dot = 2·3 + 2·0 + 2·1 + 2·2 = 12
cos = 12 / (4√14)

Which is closest?

Compute approximate values:

D1–D2 ≈ 6 / (6.928) = 0.866

D1–D3 ≈ 10 / (13.266) = 0.754

D1–D4 ≈ 12 / (14.966) = 0.802

Closest = D2

How cosine similarity helps

Cosine similarity captures how directionally similar two documents are, making it effective for clustering documents with similar term usage regardless of length.

SECTION C — Analytical Questions
Q11. Differences between multilingual clustering via PARAFAC2 and LSA.

PARAFAC2:

Tensor-based; handles multiple languages as independent but related slices.

Deals with variable term sets across languages.

Models cross-lingual coherence by decomposing term–document–language tensors.

Effective for corpora with inconsistent vocabulary sizes.

Traditional LSA:

Matrix-based SVD on concatenated multilingual corpus.

Requires vocabulary alignment across languages.

Struggles when cross-lingual homonyms or mismatched term frequencies exist.

Less effective under linguistic drift or inconsistent corpora.

Conclusion:
PARAFAC2 provides more robust multilingual clustering because it handles varying term distributions and cross-lingual alignment implicitly.

Q12. Effectiveness of VSM, LSA, LDA, Online LDA for temporal drift.

VSM:

Static representation; no adaptation to time.

Weak under drifting vocabularies.

LSA:

Captures latent structure but must be recomputed fully when data evolves.

Not suitable for streaming scenarios.

Batch LDA:

Captures topic distributions but requires retraining if new data arrives with new themes.

Online LDA:

Incrementally updates topic distributions as new documents arrive.

Adapts to topic evolution, emerging vocabulary, and temporal drift.

Most reliable:
Online LDA because it maintains topic coherence while continuously updating model parameters.

SECTION D — Case Study
Q14
(a) How RAKE extracts keywords

RAKE works by:

Splitting text into candidate phrases using stop-words as boundaries.

Calculating word scores using:
score(word) = degree(word) / frequency(word).

Scoring each phrase by summing individual word scores.

Ranking phrases to identify top candidate keywords.

(b) Impact of excluding domain-specific stop-words

If domain-specific stop-words (e.g., “breaking,” “news,” “update”) are not removed:

RAKE will generate noisy multi-word keyphrases.

Precision decreases because phrases dominated by generic newsroom vocabulary are selected.

Domain noise overwhelms meaningful topic-indicative terms.

Thus excluding domain-specific stop-words improves accuracy.

(c) Precision, Recall, F1-Score

Given:
Extracted = 20
Relevant among extracted = 12
Gold relevant = 15

Precision = 12/20 = 0.6
Recall = 12/15 = 0.8
F1 = 2 × (0.6×0.8)/(0.6+0.8)
= 0.6857

(d) Should RAKE alone be used?

Recommendation:

RAKE efficiently extracts candidate phrases but lacks semantic understanding. When combined with embedding-based ranking (e.g., sentence-BERT), noise is reduced and keyword relevance improves significantly.

Thus a hybrid RAKE + embedding-based ranking approach is preferred for high-quality keyword extraction in news streams.


  
</body>
</html>






