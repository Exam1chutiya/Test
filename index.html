<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Text Mining Exam Answers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 40px;
            background: #f8f8f8;
        }
        h1, h2, h3 {
            color: #333;
        }
        .section {
            background: #ffffff;
            padding: 20px;
            margin-bottom: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .question {
            margin-top: 20px;
        }
        b {
            color: #222;
        }
    </style>
</head>
<body>

<h1>Text Mining – Complete Exam Answers</h1>

<div class="section">
    <h2>SECTION A (20–30 Words Answers)</h2>

    <div class="question">
        <p><b>Q A1. Define candidate keywords in RAKE.</b></p>
        <p>Candidate keywords are contiguous sequences of non-stopwords extracted from a text after removing stopwords and punctuation. These meaningful word groups act as potential keyword phrases for scoring.</p>
    </div>

    <div class="question">
        <p><b>Q A2. Purpose of a stop list in keyword extraction.</b></p>
        <p>A stop list removes very common, low-information words such as “is”, “the”, and “of” so that keyword extraction focuses on meaningful, content-rich phrases only.</p>
    </div>

    <div class="question">
        <p><b>Q A3. Difference between precision and recall.</b></p>
        <p>Precision measures how many extracted keywords are correct. Recall measures how many of the true keywords are successfully extracted by the system.</p>
    </div>
</div>

<div class="section">
    <h2>SECTION B (150–200 Words Answers)</h2>

    <div class="question">
        <p><b>Q B1. RAKE Computation</b></p>
        <p><b>a) Candidate keywords after removing stopwords:</b><br>
            text mining, process, extracting useful knowledge, unstructured data, keyword extraction, important task, text mining</p>

        <p><b>b) Co-occurrence graph:</b><br>
            text–mining, extracting–useful–knowledge, unstructured–data, keyword–extraction, important–task</p>

        <p><b>c) Degree & frequency table:</b></p>
        <ul>
            <li>text: freq 2, degree 2</li>
            <li>mining: 2, 2</li>
            <li>process: 1, 0</li>
            <li>extracting: 1, 2</li>
            <li>useful: 1, 2</li>
            <li>knowledge: 1, 2</li>
            <li>unstructured: 1, 1</li>
            <li>data: 1, 1</li>
            <li>keyword: 1, 1</li>
            <li>extraction: 1, 1</li>
            <li>important: 1, 1</li>
            <li>task: 1, 1</li>
        </ul>

        <p><b>d) RAKE scores:</b><br>
        text mining = 2<br>
        extracting useful knowledge = 6 (highest)<br>
        unstructured data = 2<br>
        keyword extraction = 2<br>
        important task = 2</p>
    </div>

    <div class="question">
        <p><b>Q B2. Need for evaluating keyword extraction on new articles</b></p>
        <p>Evaluating keyword extraction algorithms on new articles is important because language, writing styles, and topical vocabulary change over time. As new domains and terminology emerge, older models may fail to capture relevant keywords. Continuous evaluation ensures that extracted keywords stay accurate, up-to-date, and representative of the article’s core meaning. It also detects issues such as bias, topic drift, and domain dependency, which can reduce performance on new content. Evaluation helps maintain robustness, ensuring the system works effectively for diverse content types such as blogs, news, and technical reports. Without fresh evaluation, models become outdated and lose reliability.</p>
    </div>

    <div class="question">
        <p><b>Q B3. SVD & LSA Usage</b></p>
        <p><b>a)</b> SVD decomposes the term-document matrix into U, Σ, and Vᵀ. A rank-2 approximation retains only the top two singular values, capturing the most important semantic structure while removing noise.</p>
        <p><b>b)</b> LSA uses this reduced semantic space to measure document similarity. Even if documents do not share exact terms, LSA groups them based on latent relationships. Thus D1 and D2 become closer because they share semantic context, making similarity detection more accurate.</p>
    </div>

    <div class="question">
        <p><b>Q B4. Tucker1 Method in Multilingual Clustering</b></p>
        <p>The Tucker1 method decomposes multilingual data stored as a 3-way tensor (terms × documents × languages). It captures shared semantic features across languages by extracting a low-dimensional core representation. This representation aligns multilingual documents in a unified semantic space, enabling cross-lingual comparison. Dimensionality reduction removes noise and linguistic variations, improving clustering quality. Tucker1 reveals hidden semantic similarities between documents even when written in different languages.</p>
    </div>
</div>

<div class="section">
    <h2>SECTION C (300–350 Words Answer)</h2>

    <div class="question">
        <p><b>Q C1. Role of Text Visualization in Anomaly & Trend Detection</b></p>
        <p>Text visualization techniques help analysts identify anomalies, emerging topics, and long-term trends within large textual datasets. Tag clouds visually emphasize frequently appearing terms, allowing sudden spikes or drops to be detected easily. For instance, an unexpected rise in words such as “breach” or “attack” in log data may indicate a cybersecurity anomaly.</p>

        <p>Authorship analysis visualizations highlight structural writing patterns such as sentence length, vocabulary diversity, and stylometric features. These can expose anomalies like plagiarism, impersonation, or unusual writing behavior. Trend lines and temporal heatmaps show how keyword frequencies evolve over time, helping track thematic progression in news, social media, or research documents.</p>

        <p>Change-tracking visualizations identify modifications across evolving text collections, making concept drift easier to detect. For example, during a pandemic, the shift from “outbreak” to “lockdown” to “vaccination” can be visualized clearly.</p>

        <p>Advanced tools such as topic maps, document networks, and t-SNE visual projections help uncover hidden clusters, semantic relationships, and outlier documents. These visualizations simplify complex text analysis, enabling faster decision-making.</p>
    </div>
</div>

</body>
</html>
